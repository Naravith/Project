{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from datetime import datetime\n",
    "from ryu.app import simple_switch_13\n",
    "from ryu.controller import ofp_event\n",
    "from ryu.controller.handler import MAIN_DISPATCHER, DEAD_DISPATCHER\n",
    "from ryu.controller.handler import set_ev_cls\n",
    "from ryu.lib import hub\n",
    "\n",
    "\n",
    "class SimpleMonitor13(simple_switch_13.SimpleSwitch13):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SimpleMonitor13, self).__init__(*args, **kwargs)\n",
    "        self.datapaths = {}\n",
    "        self.monitor_thread = hub.spawn(self._monitor)\n",
    "    self.fields = {'time':'','datapath':'','in-port':'','eth_src':'','eth_dst':'','out-port':'','total_packets':0,'total_bytes':0}\n",
    "\n",
    "    @set_ev_cls(ofp_event.EventOFPStateChange,\n",
    "                [MAIN_DISPATCHER, DEAD_DISPATCHER])\n",
    "    def _state_change_handler(self, ev):\n",
    "        datapath = ev.datapath\n",
    "        if ev.state == MAIN_DISPATCHER:\n",
    "            if datapath.id not in self.datapaths:\n",
    "                self.logger.debug('register datapath: %016x', datapath.id)\n",
    "                self.datapaths[datapath.id] = datapath\n",
    "        elif ev.state == DEAD_DISPATCHER:\n",
    "            if datapath.id in self.datapaths:\n",
    "                self.logger.debug('unregister datapath: %016x', datapath.id)\n",
    "                del self.datapaths[datapath.id]\n",
    "\n",
    "    def _monitor(self):\n",
    "        self.logger.info('time\\tdatapath\\tin-port\\teth-src\\teth-dst\\tout-port\\ttotal_packets\\ttotal_bytes')\n",
    "        while True:\n",
    "            for dp in self.datapaths.values():\n",
    "                self._request_stats(dp)\n",
    "            hub.sleep(1)\n",
    "\n",
    "    def _request_stats(self, datapath):\n",
    "        self.logger.debug('send stats request: %016x', datapath.id)\n",
    "        ofproto = datapath.ofproto\n",
    "        parser = datapath.ofproto_parser\n",
    "\n",
    "        req = parser.OFPFlowStatsRequest(datapath)\n",
    "        datapath.send_msg(req)\n",
    "\n",
    "        req = parser.OFPPortStatsRequest(datapath, 0, ofproto.OFPP_ANY)\n",
    "        datapath.send_msg(req)\n",
    "\n",
    "    @set_ev_cls(ofp_event.EventOFPFlowStatsReply, MAIN_DISPATCHER)\n",
    "    def _flow_stats_reply_handler(self, ev):\n",
    "        body = ev.msg.body\n",
    "\n",
    "        for stat in sorted([flow for flow in body if flow.priority == 1],\n",
    "                           key=lambda flow: (flow.match['in_port'],\n",
    "                                             flow.match['eth_dst'])):\n",
    "            #print details of flows\n",
    "            self.fields['time'] = datetime.utcnow().strftime('%s')\n",
    "            self.fields['datapath'] = ev.msg.datapath.id\n",
    "            self.fields['in-port'] = stat.match['in_port']\n",
    "            self.fields['eth_src'] = stat.match['eth_src']\n",
    "            self.fields['eth_dst'] = stat.match['eth_dst']\n",
    "            self.fields['out-port'] = stat.instructions[0].actions[0].port\n",
    "            self.fields['total_packets'] = stat.packet_count\n",
    "            self.fields['total_bytes'] = stat.byte_count\n",
    "\n",
    "            self.logger.info('data\\t%s\\t%x\\t%x\\t%s\\t%s\\t%x\\t%d\\t%d',self.fields['time'],self.fields['datapath'],self.fields['in-port'],self.fields['eth_src'],self.fields['eth_dst'],self.fields['out-port'],self.fields['total_packets'],self.fields['total_bytes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "from prettytable import PrettyTable #to display output from ML model\n",
    "import subprocess, sys #to handle the Ryu output\n",
    "import signal #for timer\n",
    "import os #for process handling\n",
    "import numpy as np #for model features\n",
    "import pickle #to use ML model real-time\n",
    "\n",
    "## command to run ##\n",
    "cmd = \"sudo ryu run /usr/local/lib/python2.7/dist-packages/ryu/app/simple_monitor_AK.py\"\n",
    "flows = {} #empty flow dictionary\n",
    "TIMEOUT = 15*60 #15 min #how long to collect training data\n",
    "\n",
    "class Flow:\n",
    "    def __init__(self, time_start, datapath, inport, ethsrc, ethdst, outport, packets, bytes):\n",
    "        self.time_start = time_start\n",
    "        self.datapath = datapath\n",
    "        self.inport = inport\n",
    "        self.ethsrc = ethsrc\n",
    "        self.ethdst = ethdst\n",
    "        self.outport = outport\n",
    "        \n",
    "        #attributes for forward flow direction (source -> destination)\n",
    "        self.forward_packets = packets\n",
    "        self.forward_bytes = bytes\n",
    "        self.forward_delta_packets = 0\n",
    "        self.forward_delta_bytes = 0\n",
    "        self.forward_inst_pps = 0.00\n",
    "        self.forward_avg_pps = 0.00\n",
    "        self.forward_inst_bps = 0.00\n",
    "        self.forward_avg_bps = 0.00\n",
    "        self.forward_status = 'ACTIVE'\n",
    "        self.forward_last_time = time_start\n",
    "        \n",
    "        #attributes for reverse flow direction (destination -> source)\n",
    "        self.reverse_packets = 0\n",
    "        self.reverse_bytes = 0\n",
    "        self.reverse_delta_packets = 0\n",
    "        self.reverse_delta_bytes = 0\n",
    "        self.reverse_inst_pps = 0.00\n",
    "        self.reverse_avg_pps = 0.00\n",
    "        self.reverse_inst_bps = 0.00\n",
    "        self.reverse_avg_bps = 0.00\n",
    "        self.reverse_status = 'INACTIVE'\n",
    "        self.reverse_last_time = time_start\n",
    "        \n",
    "    #updates the attributes in the forward flow direction\n",
    "    def updateforward(self, packets, bytes, curr_time):\n",
    "        self.forward_delta_packets = packets - self.forward_packets\n",
    "        self.forward_packets = packets\n",
    "        if curr_time != self.time_start: self.forward_avg_pps = packets/float(curr_time-self.time_start)\n",
    "        if curr_time != self.forward_last_time: self.forward_inst_pps = self.forward_delta_packets/float(curr_time-self.forward_last_time)\n",
    "        \n",
    "        self.forward_delta_bytes = bytes - self.forward_bytes\n",
    "        self.forward_bytes = bytes\n",
    "        if curr_time != self.time_start: self.forward_avg_bps = bytes/float(curr_time-self.time_start)\n",
    "        if curr_time != self.forward_last_time: self.forward_inst_bps = self.forward_delta_bytes/float(curr_time-self.forward_last_time)\n",
    "        self.forward_last_time = curr_time\n",
    "        \n",
    "        if (self.forward_delta_bytes==0 or self.forward_delta_packets==0): #if the flow did not receive any packets of bytes\n",
    "            self.forward_status = 'INACTIVE'\n",
    "        else:\n",
    "            self.forward_status = 'ACTIVE'\n",
    "\n",
    "    #updates the attributes in the reverse flow direction\n",
    "    def updatereverse(self, packets, bytes, curr_time):\n",
    "        self.reverse_delta_packets = packets - self.reverse_packets\n",
    "        self.reverse_packets = packets\n",
    "        if curr_time != self.time_start: self.reverse_avg_pps = packets/float(curr_time-self.time_start)\n",
    "        if curr_time != self.reverse_last_time: self.reverse_inst_pps = self.reverse_delta_packets/float(curr_time-self.reverse_last_time)\n",
    "        \n",
    "        self.reverse_delta_bytes = bytes - self.reverse_bytes\n",
    "        self.reverse_bytes = bytes\n",
    "        if curr_time != self.time_start: self.reverse_avg_bps = bytes/float(curr_time-self.time_start)\n",
    "        if curr_time != self.reverse_last_time: self.reverse_inst_bps = self.reverse_delta_bytes/float(curr_time-self.reverse_last_time)\n",
    "        self.reverse_last_time = curr_time\n",
    "\n",
    "        if (self.reverse_delta_bytes==0 or self.reverse_delta_packets==0): #if the flow did not receive any packets of bytes\n",
    "            self.reverse_status = 'INACTIVE'\n",
    "        else:\n",
    "            self.reverse_status = 'ACTIVE'\n",
    "\n",
    "#function to print flow attributes and output of ML model to classify the flow\n",
    "def printclassifier(model):\n",
    "    x = PrettyTable()\n",
    "    x.field_names = [\"Flow ID\", \"Src MAC\", \"Dest MAC\", \"Traffic Type\",\"Forward Status\",\"Reverse Status\"]\n",
    "\n",
    "    for key,flow in flows.items():\n",
    "        features = np.asarray([flow.forward_delta_packets,flow.forward_delta_bytes,flow.forward_inst_pps,flow.forward_avg_pps,flow.forward_inst_bps, flow.forward_avg_bps, flow.reverse_delta_packets,flow.reverse_delta_bytes,flow.reverse_inst_pps,flow.reverse_avg_pps,flow.reverse_inst_bps,flow.reverse_avg_bps]).reshape(1,-1) #convert to array so the model can understand the features properly\n",
    "        \n",
    "        label = model.predict(features.tolist()) #if model is supervised (logistic regression) then the label is the type of traffic\n",
    "        \n",
    "        #if the model is unsupervised, the label is a cluster number. Refer to Jupyter notebook to see how cluster numbers map to labels\n",
    "        if label == 0: label = ['dns']\n",
    "        elif label == 1: label = ['ping']\n",
    "        elif label == 2: label = ['telnet']\n",
    "        elif label == 3: label = ['voice']\n",
    "        \n",
    "        x.add_row([key, flow.ethsrc, flow.ethdst, label[0],flow.forward_status,flow.reverse_status]) \n",
    "    print(x)#print output in pretty mode (i.e. formatted table)\n",
    "\n",
    "#function to print flow attributes when collecting training data\n",
    "def printflows(traffic_type,f):\n",
    "    for key,flow in flows.items():\n",
    "        outstring = '\\t'.join([\n",
    "        str(flow.forward_packets),\n",
    "        str(flow.forward_bytes),\n",
    "        str(flow.forward_delta_packets),\n",
    "        str(flow.forward_delta_bytes), \n",
    "        str(flow.forward_inst_pps), \n",
    "        str(flow.forward_avg_pps),\n",
    "        str(flow.forward_inst_bps), \n",
    "        str(flow.forward_avg_bps), \n",
    "        str(flow.reverse_packets),\n",
    "        str(flow.reverse_bytes),\n",
    "        str(flow.reverse_delta_packets),\n",
    "        str(flow.reverse_delta_bytes),\n",
    "        str(flow.reverse_inst_pps),\n",
    "        str(flow.reverse_avg_pps),\n",
    "        str(flow.reverse_inst_bps),\n",
    "        str(flow.reverse_avg_bps),\n",
    "        str(traffic_type)])\n",
    "        f.write(outstring+'\\n')\n",
    "        \n",
    "def run_ryu(p,traffic_type=None,f=None,model=None):\n",
    "    ## run it ##\n",
    "    time = 0\n",
    "    while True:\n",
    "        #print 'going through loop'\n",
    "        out = p.stdout.readline()\n",
    "        if out == '' and p.poll() != None:\n",
    "            break\n",
    "        if out != '' and out.startswith(b'data'): #when Ryu 'simple_monitor_AK.py' script returns output\n",
    "            fields = out.split(b'\\t')[1:] #split the flow details\n",
    "            \n",
    "            fields = [f.decode(encoding='utf-8', errors='strict') for f in fields] #decode flow details \n",
    "            \n",
    "            unique_id = hash(''.join([fields[1],fields[3],fields[4]])) #create unique ID for flow based on switch ID, source host,and destination host\n",
    "            if unique_id in flows.keys():\n",
    "                flows[unique_id].updateforward(int(fields[6]),int(fields[7]),int(fields[0])) #update forward attributes with time, packet, and byte count\n",
    "            else:\n",
    "                rev_unique_id = hash(''.join([fields[1],fields[4],fields[3]])) #switch source and destination to generate same hash for src/dst and dst/src\n",
    "                if rev_unique_id in flows.keys():\n",
    "                    flows[rev_unique_id].updatereverse(int(fields[6]),int(fields[7]),int(fields[0])) #update reverse attributes with time, packet, and byte count\n",
    "                else:\n",
    "                    flows[unique_id] = Flow(int(fields[0]), fields[1], fields[2], fields[3], fields[4], fields[5], int(fields[6]), int(fields[7])) #create new flow object\n",
    "            if not model is None:\n",
    "                if time%10==0: #print output of model every 10 seconds\n",
    "                    printclassifier(model)\n",
    "            else:\n",
    "                printflows(traffic_type,f) #for training data\n",
    "        time += 1\n",
    " \n",
    "#print help output in case of incorrect options \n",
    "def printHelp():\n",
    "    print(\"Usage: sudo python traffic_classifier.py [subcommand] [options]\")\n",
    "    print(\"\\tTo collect training data for a certain type of traffic, run: sudo python traffic_classifier.py train [voice|video|ftp]\")\n",
    "    print(\"\\tTo start a near real time traffic classification application using unsupervised ML, run: sudo python traffic_classifier.py unsupervised\")\n",
    "    print(\"\\tTo start a near real time traffic classification application using supervised ML, run: sudo python traffic_classifier.py unsupervised\")\n",
    "    return\n",
    "\n",
    "#for timer to collect flow training data\n",
    "def alarm_handler(signum, frame):\n",
    "    print(\"Finished collecting data.\")\n",
    "    raise Exception()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    SUBCOMMANDS = ('train', 'unsupervised', 'supervised')\n",
    "\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"ERROR: Incorrect # of args\")\n",
    "        print()\n",
    "        printHelp()\n",
    "        sys.exit();\n",
    "    else:\n",
    "        if len(sys.argv) == 2:\n",
    "            if sys.argv[1] not in SUBCOMMANDS:\n",
    "                print(\"ERROR: Unknown subcommand argument.\")\n",
    "                print(\"       Currently subaccepted commands are: %s\" % str(SUBCOMMANDS).strip('()'))\n",
    "                print()\n",
    "                printHelp()\n",
    "                sys.exit();\n",
    "\n",
    "    if len(sys.argv) == 1:\n",
    "        # Called with no arguments\n",
    "        printHelp()\n",
    "    elif len(sys.argv) >= 2:\n",
    "        if sys.argv[1] == \"train\":\n",
    "            if len(sys.argv) == 3:\n",
    "                p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) #start Ryu process\n",
    "                traffic_type = sys.argv[2]\n",
    "                f = open(traffic_type+'_training_data.csv', 'w') #open training data output file\n",
    "                signal.signal(signal.SIGALRM, alarm_handler) #start signal process\n",
    "                signal.alarm(TIMEOUT) #set for 15 minutes\n",
    "                try:\n",
    "                    headers = 'Forward Packets\\tForward Bytes\\tDelta Forward Packets\\tDelta Forward Bytes\\tForward Instantaneous Packets per Second\\tForward Average Packets per second\\tForward Instantaneous Bytes per Second\\tForward Average Bytes per second\\tReverse Packets\\tReverse Bytes\\tDelta Reverse Packets\\tDelta Reverse Bytes\\tDeltaReverse Instantaneous Packets per Second\\tReverse Average Packets per second\\tReverse Instantaneous Bytes per Second\\tReverse Average Bytes per second\\tTraffic Type\\n'\n",
    "                    f.write(headers)\n",
    "                    run_ryu(p,traffic_type=traffic_type,f=f)\n",
    "                except Exception:\n",
    "                    print('Exiting')\n",
    "                    os.killpg(os.getpgid(p.pid), signal.SIGTERM) #kill ryu process on exit\n",
    "                    f.close()\n",
    "            else:\n",
    "                print(\"ERROR: specify traffic type.\\n\")\n",
    "                printHelp()\n",
    "        else:\n",
    "            p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT) #start ryu process\n",
    "            if sys.argv[1] == 'supervised':\n",
    "                infile = open('LogisticRegression','rb') \n",
    "            elif sys.argv[1] == 'unsupervised':\n",
    "                infile = open('KMeans_Clustering','rb')\n",
    "            model = pickle.load(infile) #unload previously trained ML model (refer to Jupyter notebook for details)\n",
    "            infile.close()\n",
    "            run_ryu(p,model=model)\n",
    "    sys.exit();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
